# Example environment variables for backend configuration
# LLM Provider selection: ollama, openai, openrouter, gemini, anthropic
API_PROVIDER=openai

# Model selection (for all providers)
PROVIDER_MODEL=gpt-3.5-turbo

# Common LLM settings
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1024
LLM_STOP_WORD=

# Ollama settings (for local LLM)
OLLAMA_HOST=ollama
OLLAMA_PORT=11434

# OpenAI API settings
OPENAI_API_KEY=your-openai-api-key

# OpenRouter API settings
OPENROUTER_API_KEY=your-openrouter-api-key

# Gemini API settings
GEMINI_API_KEY=your-gemini-api-key

# Anthropic API settings
ANTHROPIC_API_KEY=your-anthropic-api-key

# System prompt for the assistant (optional, overrides default prompt in backend)
SYSTEM_PROMPT=You are a helpful customer support assistant for an online store called 'ShopEasy'. You help customers with product inquiries, order status, return policies, and general shopping assistance. Keep responses brief, friendly, and helpful. If you don't know something, admit it and offer to connect the customer with a human agent.

# Debug mode
DEBUG=False
